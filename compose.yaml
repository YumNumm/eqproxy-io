x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    tag: "{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}"

services:
  eqproxy-io:
    build:
      context: .
      target: eqproxy-io
    env_file:
      - .env
    environment:
      - TZ=Asia/Tokyo
      - PORT=4000
    restart: always
    ports:
      # ws.api.eqmonitor.app
      - 4000:4000
      # Prometheus metrics
      - 4002:4002

  dmdata-proxy:
    build:
      context: .
      target: dmdata-proxy
    env_file:
      - ./service/dmdata-proxy/.env
    environment:
      - TZ=Asia/Tokyo
      - WEBSOCKET_PORT=4020
      - NEW_RELIC_APP_NAME=dmdata-proxy
    restart: always
    ports:
      - 4020:4020
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4020/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s

  supabase-proxy:
    build: ./service/supabase-proxy
    env_file:
      - ./service/supabase-proxy/.env
    environment:
      - TZ=Asia/Tokyo
      - PORT=4010
    restart: always
    ports:
      - 4010:4010
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4010/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s

  telegram-proxy-server:
    build:
      context: .
      dockerfile: ./server/telegram-proxy-server/Dockerfile
    env_file:
      - ./server/telegram-proxy-server/.env
    environment:
      - TZ=Asia/Tokyo
      - PORT=4030
      - SUPABASE_PROXY_URL=ws://supabase-proxy:4010/ws
      - DMDATA_PROXY_URL=ws://dmdata-proxy:4020/ws
    restart: always
    depends_on:
      supabase-proxy:
        condition: service_healthy
      dmdata-proxy:
        condition: service_healthy
    ports:
      - 4030:4030
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4030/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s

  telegram-proxy-server-dev:
    build:
      context: .
      dockerfile: ./server/telegram-proxy-server-dev/Dockerfile
    environment:
      - TZ=Asia/Tokyo
      - PORT=4031
    restart: always
    ports:
      - 4031:4031
    volumes:
      - ./server/telegram-proxy-server-dev/assets:/usr/src/app/assets
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4031/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s

  db:
    container_name: supabase-db
    build: ./db
    healthcheck:
      test: pg_isready -U postgres -h localhost
      interval: 5s
      timeout: 5s
      retries: 10
    command:
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=error # prevents Realtime polling queries from appearing in logs
    restart: unless-stopped
    ports:
      # Pass down internal port because it's set dynamically by other services
      - ${POSTGRES_PORT}:${POSTGRES_PORT}
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXP: ${JWT_EXPIRY}
      POSTGRES_INITDB_ARGS: --locale=ja_JP.utf8
      TZ: Asia/Tokyo
    volumes:
      # Must be superuser to alter reserved role
      - ./db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      # PGDATA directory is persisted between restarts
      - ./db/data:/var/lib/postgresql/data:Z
      - ./db/init/data.sql:/docker-entrypoint-initdb.d/migrations/99-data.sql:Z
      # Use named volume to persist pgsodium decryption key between restarts
      - db-config:/etc/postgresql-custom
      - ./db/postgresql.conf:/etc/postgresql/postgresql.conf:Z

  postgres-exporter:
    container_name: postgres-exporter
    image: quay.io/prometheuscommunity/postgres-exporter
    hostname: postgres-exporter
    env_file:
      - .env
    ports:
      - 9187:9187

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:3.12-management-alpine
    environment:
      RABBITMQ_ERLANG_COOKIE: "SWQOKODSQALRPCLNMEQG"
      RABBITMQ_DEFAULT_USER: "rabbitmq"
      RABBITMQ_DEFAULT_PASS: "rabbitmq"
      RABBITMQ_DEFAULT_VHOST: "/"
    healthcheck:
      test: rabbitmq-diagnostics check_port_connectivity
      interval: 1s
      timeout: 3s
      retries: 30
    cap_add:
      - ALL
    ports:
      - 15672:15672
      - 15692:15692
      - 5672:5672
    labels:
      NAME: "rabbitmq1"
    volumes:
      - ./rabbitmq:/etc/rabbitmq:Z

  notification-producer:
    container_name: notification-producer
    build:
      context: .
      target: notification-producer
    environment:
      - TZ=Asia/Tokyo
      - RABBITMQ_HOST=rabbitmq
    env_file:
      - .env
    restart: always
    depends_on:
      rabbitmq:
        condition: service_healthy

  notification-worker:
    container_name: notification-worker
    build:
      context: .
      target: notification-worker
    environment:
      - TZ=Asia/Tokyo
      - RABBITMQ_HOST=rabbitmq
      - GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json
    env_file:
      - .env
    restart: always
    volumes:
      - ./service/notification-worker/credentials.json:/app/credentials.json:ro
    depends_on:
      rabbitmq:
        condition: service_healthy
    extra_hosts:
      - mac-mini:100.117.92.56

  fcm-token-verify:
    build:
      context: .
      target: fcm-token-verify
    environment:
      - TZ=Asia/Tokyo
      - PORT=3000
    env_file:
      - .env
    ports:
      - 8020:3000
    volumes:
      - ./server/fcm-token-verify/credentials.json:/prod/server/fcm-token-verify/credentials.json:ro

  # gorush:
  #   image: appleboy/gorush
  #   restart: always
  #   volumes:
  #     - ./config/gorush/gorush.yaml:/home/gorush/config.yml
  #     - ./config/gorush/credentials.json:/home/gorush/credentials.json
  #   ports:
  #     - "8088:8088"
  #     - "9000:9000"

volumes:
  db-config:
