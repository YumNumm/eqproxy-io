x-default-logging: &logging
  driver: "json-file"
  options:
    max-size: "5m"
    max-file: "2"
    tag: "{{.ImageName}}|{{.Name}}|{{.ImageFullID}}|{{.FullID}}"

services:
  eqproxy-io:
    image: eqproxy-io
    build:
      context: .
      dockerfile: server/eqproxy-io/Dockerfile
    env_file:
      - .env
    environment:
      - TZ=Asia/Tokyo
      - PORT=4000
    restart: always
    ports:
      # ws.api.eqmonitor.app
      - 4000:4000
      # Prometheus metrics
      - 4002:4002
    networks:
      - external

  dmdata-proxy:
    image: dmdata-proxy
    build:
      context: .
      dockerfile: service/dmdata-proxy/Dockerfile
    env_file:
      - ./service/dmdata-proxy/.env
    environment:
      - TZ=Asia/Tokyo
      - WEBSOCKET_PORT=4020
      - NEW_RELIC_APP_NAME=dmdata-proxy
    restart: always
    ports:
      - 4020:4020
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4020/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - external

  supabase-proxy:
    image: supabase-proxy
    build:
      context: .
      dockerfile: service/supabase-proxy/Dockerfile
    tty: true
    entrypoint: ["bun", "run", "service/supabase-proxy/server.js"]
    env_file:
      - ./service/supabase-proxy/.env
    environment:
      - TZ=Asia/Tokyo
      - PORT=4010
    restart: always
    ports:
      - 4010:4010
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4010/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - external

  telegram-proxy-server:
    image: telegram-proxy-server
    build:
      context: .
      dockerfile: ./server/telegram-proxy-server/Dockerfile
    env_file:
      - ./server/telegram-proxy-server/.env
    environment:
      - TZ=Asia/Tokyo
      - PORT=4030
      - SUPABASE_PROXY_URL=ws://supabase-proxy:4010/ws
      - DMDATA_PROXY_URL=ws://dmdata-proxy:4020/ws
      - SHAKE_DETECT_PROXY_URL=ws://shake-detect-service:8181
    restart: always
    depends_on:
      supabase-proxy:
        condition: service_healthy
      dmdata-proxy:
        condition: service_healthy
    ports:
      - 4030:4030
    healthcheck:
      test: ["CMD", "curl", "http://localhost:4030/health"]
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - external

  # telegram-proxy-server-dev:
  #   image: telegram-proxy-server-dev
  #   build:
  #     context: .
  #     dockerfile: ./server/telegram-proxy-server-dev/Dockerfile
  #   environment:
  #     - TZ=Asia/Tokyo
  #     - PORT=4031
  #   restart: always
  #   ports:
  #     - 4031:4031
  #   volumes:
  #     - ./server/telegram-proxy-server-dev/assets:/usr/src/app/assets
  #   healthcheck:
  #     test: ["CMD", "curl", "http://localhost:4031/health"]
  #     interval: 5s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   networks:
  #     - external

  shake-reciever:
    image: shake-reciever
    build:
      context: .
      dockerfile: ./service/shake-reciever/Dockerfile
    env_file:
      - ./service/shake-reciever/.env
    environment:
      - TZ=Asia/Tokyo
      - WEBSOCKET_URL=ws://shake-detect-service:8181
      - GO_RUSH_URL=http://gorush:8088
    networks:
      - external

  db:
    container_name: supabase-db
    build: ./db
    healthcheck:
      test: pg_isready -U postgres -h localhost
      interval: 5s
      timeout: 5s
      retries: 10
    command:
      - postgres
      - -c
      - config_file=/etc/postgresql/postgresql.conf
      - -c
      - log_min_messages=error # prevents Realtime polling queries from appearing in logs
    restart: always
    ports:
      # Pass down internal port because it's set dynamically by other services
      - ${POSTGRES_PORT}:${POSTGRES_PORT}
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: ${POSTGRES_PORT}
      POSTGRES_PORT: ${POSTGRES_PORT}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: ${POSTGRES_DB}
      POSTGRES_DB: ${POSTGRES_DB}
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXP: ${JWT_EXPIRY}
      POSTGRES_INITDB_ARGS: --locale=ja_JP.utf8
      TZ: Asia/Tokyo
    volumes:
      # Must be superuser to alter reserved role
      - ./db/roles.sql:/docker-entrypoint-initdb.d/init-scripts/99-roles.sql:Z
      # PGDATA directory is persisted between restarts
      - ./db/data:/var/lib/postgresql/data:Z
      - ./db/init/data.sql:/docker-entrypoint-initdb.d/migrations/99-data.sql:Z
      # Use named volume to persist pgsodium decryption key between restarts
      - db-config:/etc/postgresql-custom
      - ./db/postgresql.conf:/etc/postgresql/postgresql.conf:Z
      - ./logs/db:/var/lib/postgresql/data/log:rw
    networks:
      - external

  postgres-exporter:
    container_name: postgres-exporter
    image: quay.io/prometheuscommunity/postgres-exporter
    hostname: postgres-exporter
    env_file:
      - .env
    ports:
      - 9187:9187
    networks:
      - external

  notification-producer:
    image: notification-producer
    container_name: notification-producer
    build:
      context: .
      dockerfile: service/notification-producer/Dockerfile
    environment:
      - TZ=Asia/Tokyo
      - RABBITMQ_HOST=rabbitmq
      - GO_RUSH_URL=http://gorush:8088
    env_file:
      - .env
    restart: always
    depends_on:
      - gorush
    networks:
      - external

  fcm-token-verify:
    image: fcm-token-verify
    build:
      context: .
      dockerfile: server/fcm-token-verify/Dockerfile
    environment:
      - TZ=Asia/Tokyo
      - PORT=3000
    env_file:
      - .env
    ports:
      - 8020:3000
    volumes:
      - ./server/fcm-token-verify/credentials.json:/prod/server/fcm-token-verify/credentials.json:ro
    networks:
      - external

  gorush:
    image: appleboy/gorush
    restart: always
    volumes:
      - ./config/gorush/gorush.yaml:/home/gorush/config.yml:ro
      - ./config/gorush/credentials.json:/home/gorush/credentials.json:ro
      - ./logs/gorush:/home/gorush/logs:rw
    ports:
      - "8088:8088"
      - "9000:9000"
    networks:
      - external

volumes:
  db-config:

networks:
  external:
